% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/scrape-listpage.R
\name{fetch_all_listpages}
\alias{fetch_all_listpages}
\title{Fetch html of multiple geizhals category pages}
\usage{
fetch_all_listpages(firstlistpageurl, max_pages = 10,
  delay_listpage = NA, domain = "https://geizhals.at")
}
\arguments{
\item{firstlistpageurl}{Character vector of length 1 containing the
url of a geizhals category page (listing all items of a selected
category).}

\item{max_pages}{Maximal number of pages to be scraped. Default is 10.}

\item{delay_listpage}{Number of seconds to wait between fetching
subsequent list pages.}

\item{domain}{Character vector of length one specifying the domain.
Defaults to \code{"https://geizhals.at"}.}
}
\value{
A list of xml documents.
}
\description{
Given the url of a geizhals page listing all products
within a specific  category (i.e., not the generic page-wide search from
the search bar, but the page showing all items within a category),
the html code from this and the following pages are returned. Filters
might be applied, only results corresponding to that filter will be
returned.
This list is meant to be processed by the \code{\link{parse_all_listpages}}
function.
}
\examples{
\dontrun{
url_geizhals <- "https://geizhals.at/?cat=acam35"
listpagehtml_list <- fetch_all_listpages(url_geizhals, max_pages = 2)
parse_all_listpages(listpagehtml_list)

url_geizhals <- "https://geizhals.eu/?cat=acam35"
listpagehtml_list <- fetch_all_listpages(url_geizhals, max_pages = 2,
  delay_listpage = 1, domain = "https://www.geizhals.eu")
parse_all_listpages(listpagehtml_list, domain = "https://www.geizhals.eu")
}

}
\seealso{
\code{\link{parse_all_listpages}}
}
